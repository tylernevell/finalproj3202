{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import gym # openai to render environment for agent to take action in and receive feedback in\n",
    "import gym.wrappers\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque # for memory of agent\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential # to build neural network for aproximating optimal Q\n",
    "from keras.layers import Dense # theta weights - weights and bias of neurons in between layers of dense network\n",
    "from keras.optimizers import Adam # stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters related to size of state and size of actions\n",
    "state_size = env.observation_space.shape[0]\n",
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter for gradient descent (vary by powers of 2)\n",
    "batch_size = 32\n",
    "n_episodes = 8000\n",
    "output_dir = \"model_output/MountainCar\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # our agent's memory\n",
    "        # prevents us from going over every single event that's happened. Too Slow!\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        \n",
    "        '''hyperparameters''' \n",
    "        self.gamma = 0.99 # discount factor of future rewards\n",
    "        \n",
    "        # two modes of action: \n",
    "            # exploitation: take best possible action based on whats been learned\n",
    "            # exploration: to explore the environment more and find new actions\n",
    "        self.epsilon = 1.0 # initial exploration rate of agent \n",
    "        self.epsilon_decay = 0.85 # slowly shifts from exploring to exploitation\n",
    "        self.epsilon_min = 0.00001 # lowest exploration percent can decay to\n",
    "        \n",
    "        self.learning_rate = 0.001251 # stochastic gradient descent step size\n",
    "        \n",
    "        self.model = self._build_model() # ensures private method can only be used by this particular instance of a class\n",
    "        self.target_model = self._build_model()\n",
    "        \n",
    "        self.update_target_model()\n",
    "        \n",
    "    def _build_model(self): # where we define dense neural network for approx Q*\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(24, input_dim = self.state_size, activation=\"relu\")) # hidden layer\n",
    "        model.add(Dense(48, activation=\"relu\")) # hidden layer        \n",
    "        \n",
    "        # output layer: as many neurons as possible actions\n",
    "        model.add(Dense(self.action_size, activation=\"linear\")) # reason for linear: we are directly modeling actions, no abstract probability\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=self.learning_rate)) # mse works for this agent, but cross-entropy might work better for others. mse is not usually first choice\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    # REALLY IMPORTANT: takes in state, action, reward, and next_state at current time step\n",
    "    # to model what will happen in next_state and what reward we can expect to receive \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    # figuring out what action to take based on state\n",
    "    def act(self, state):\n",
    "        '''\n",
    "        explore randomly or exploit information accrued in dense network\n",
    "        as epsilon decays exploit will be more likely\n",
    "        '''\n",
    "        \n",
    "        # explore\n",
    "        if np.random.rand(1) <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        #exploit\n",
    "        act_values = self.model.predict(state) # use theta weights and predict method on our model inside the agent to guess best course of action to maz future reward\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    # bulk of agent defs\n",
    "    def replay(self, batch_size):\n",
    "        \n",
    "        # samples from the deque of memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # if reached max time steps or by dying, then done\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # reward plus estimates of future reward using neural network and next state\n",
    "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)[0]))\n",
    "            \n",
    "            # map maximized future reward to current reward with theta\n",
    "            target_f= self.model.predict(state) # use neural network to estimate target given current state\n",
    "            target_f[0][action]= target # map to future state\n",
    "            \n",
    "            states.append(state[0])\n",
    "            targets.append(target_f[0])\n",
    "            \n",
    "        # train model with X: current state, Y: future reward, \n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "#             # if reached max time steps or by dying, then done\n",
    "#             target = reward\n",
    "#             if not done:\n",
    "#                 # reward plus estimates of future reward using neural network and next state\n",
    "#                 target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            \n",
    "#             # map maximized future reward to current reward with theta\n",
    "#             target_f = self.model.predict(state) # use neural network to estimate target given current state\n",
    "#             target_f[0][action] = target # map to future state\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/8000, score: 199, e 1.0, help: [[-0.57948268  0.01260537]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 1/8000, score: 199, e 0.85, help: [[-0.55856871 -0.00198235]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 2/8000, score: 199, e 0.72, help: [[-0.54974823  0.01699448]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 3/8000, score: 199, e 0.61, help: [[-0.57335211  0.01094781]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 4/8000, score: 199, e 0.52, help: [[-0.24606916  0.02434657]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 5/8000, score: 199, e 0.44, help: [[-0.51362194  0.0121166 ]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 6/8000, score: 199, e 0.38, help: [[-0.53800659  0.01649658]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 7/8000, score: 199, e 0.32, help: [[-0.68426201  0.00870184]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 8/8000, score: 199, e 0.27, help: [[-0.54446483 -0.02584037]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 9/8000, score: 199, e 0.23, help: [[-0.93327898  0.01673974]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 10/8000, score: 199, e 0.2, help: [[-0.42466864 -0.01857437]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 11/8000, score: 199, e 0.17, help: [[-0.20200018  0.00843335]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 12/8000, score: 199, e 0.14, help: [[-0.75488626 -0.00832016]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 13/8000, score: 199, e 0.12, help: [[-0.33587603  0.0079385 ]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 14/8000, score: 199, e 0.1, help: [[-0.18377562  0.05236807]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 15/8000, score: 199, e 0.087, help: [[-0.37567532  0.00170844]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 16/8000, score: 199, e 0.074, help: [[-0.04141315 -0.00550256]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 17/8000, score: 199, e 0.063, help: [[-0.39533572 -0.01293545]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 18/8000, score: 199, e 0.054, help: [[-0.41939449  0.00153263]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 19/8000, score: 199, e 0.046, help: [[-0.80899365 -0.03166184]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 20/8000, score: 199, e 0.039, help: [[-0.76684792 -0.00196148]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 21/8000, score: 199, e 0.033, help: [[-0.4431997  -0.03632293]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 22/8000, score: 199, e 0.028, help: [[-0.38577148  0.03144011]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 23/8000, score: 199, e 0.024, help: [[-0.80404148 -0.02332089]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 24/8000, score: 199, e 0.02, help: [[-0.53683665  0.02060616]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 25/8000, score: 199, e 0.017, help: [[0.19914213 0.01004229]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 26/8000, score: 199, e 0.015, help: [[ 0.09408127 -0.01658129]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 27/8000, score: 199, e 0.012, help: [[-0.55037521 -0.01604301]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 28/8000, score: 199, e 0.011, help: [[-0.93545375 -0.02144173]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 29/8000, score: 199, e 0.009, help: [[-0.30525827 -0.02301325]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 30/8000, score: 199, e 0.0076, help: [[-0.74436899  0.03167263]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 31/8000, score: 199, e 0.0065, help: [[-0.18925546  0.01633103]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 32/8000, score: 199, e 0.0055, help: [[-0.73092524  0.01863772]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 33/8000, score: 199, e 0.0047, help: [[-0.86267051 -0.01962181]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 34/8000, score: 199, e 0.004, help: [[-0.58947066  0.04240493]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 35/8000, score: 199, e 0.0034, help: [[-0.2830925   0.05299196]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 36/8000, score: 199, e 0.0029, help: [[-0.38202196 -0.02868703]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 37/8000, score: 190, e 0.0024, help: [[0.50704153 0.01902726]], reward: -1.0, 100score avg: -198.76315789473685\n",
      "episode: 38/8000, score: 199, e 0.0021, help: [[ 0.11751802 -0.02246478]], reward: -1.0, 100score avg: -198.76923076923077\n",
      "episode: 39/8000, score: 199, e 0.0018, help: [[0.15084021 0.00899945]], reward: -1.0, 100score avg: -198.775\n",
      "episode: 40/8000, score: 135, e 0.0015, help: [[0.51068721 0.03391312]], reward: -1.0, 100score avg: -197.21951219512195\n",
      "episode: 41/8000, score: 185, e 0.0013, help: [[0.51554354 0.03701044]], reward: -1.0, 100score avg: -196.92857142857142\n",
      "episode: 42/8000, score: 199, e 0.0011, help: [[-0.20350019  0.00764017]], reward: -1.0, 100score avg: -196.97674418604652\n",
      "episode: 43/8000, score: 199, e 0.00092, help: [[-0.88174237 -0.03732671]], reward: -1.0, 100score avg: -197.02272727272728\n",
      "episode: 44/8000, score: 199, e 0.00078, help: [[-0.46100885  0.01949622]], reward: -1.0, 100score avg: -197.06666666666666\n",
      "episode: 45/8000, score: 179, e 0.00067, help: [[0.51459403 0.0430353 ]], reward: -1.0, 100score avg: -196.67391304347825\n",
      "episode: 46/8000, score: 199, e 0.00057, help: [[-0.4245006   0.00842527]], reward: -1.0, 100score avg: -196.72340425531914\n",
      "episode: 47/8000, score: 199, e 0.00048, help: [[0.25946818 0.02265283]], reward: -1.0, 100score avg: -196.77083333333334\n",
      "episode: 48/8000, score: 199, e 0.00041, help: [[-0.58206095  0.0012308 ]], reward: -1.0, 100score avg: -196.81632653061226\n",
      "episode: 49/8000, score: 145, e 0.00035, help: [[0.51025771 0.01361922]], reward: -1.0, 100score avg: -195.78\n",
      "episode: 50/8000, score: 199, e 0.0003, help: [[-0.32221693  0.04385197]], reward: -1.0, 100score avg: -195.84313725490196\n",
      "episode: 51/8000, score: 199, e 0.00025, help: [[-0.7181282  -0.01121211]], reward: -1.0, 100score avg: -195.90384615384616\n",
      "episode: 52/8000, score: 199, e 0.00021, help: [[-0.18142973  0.01672829]], reward: -1.0, 100score avg: -195.96226415094338\n",
      "episode: 53/8000, score: 199, e 0.00018, help: [[-0.03875781  0.00817675]], reward: -1.0, 100score avg: -196.0185185185185\n",
      "episode: 54/8000, score: 199, e 0.00015, help: [[-0.2339081  -0.01758427]], reward: -1.0, 100score avg: -196.07272727272726\n",
      "episode: 55/8000, score: 199, e 0.00013, help: [[-0.6708235  -0.01556002]], reward: -1.0, 100score avg: -196.125\n",
      "episode: 56/8000, score: 199, e 0.00011, help: [[-0.11442863 -0.01385317]], reward: -1.0, 100score avg: -196.17543859649123\n",
      "episode: 57/8000, score: 199, e 9.5e-05, help: [[-0.18063351 -0.00309951]], reward: -1.0, 100score avg: -196.22413793103448\n",
      "episode: 58/8000, score: 199, e 8.1e-05, help: [[0.44549014 0.04697883]], reward: -1.0, 100score avg: -196.27118644067798\n",
      "episode: 59/8000, score: 199, e 6.9e-05, help: [[-0.01683947 -0.02069477]], reward: -1.0, 100score avg: -196.31666666666666\n",
      "episode: 60/8000, score: 199, e 5.8e-05, help: [[-0.04549836  0.02953788]], reward: -1.0, 100score avg: -196.36065573770492\n",
      "episode: 61/8000, score: 199, e 4.9e-05, help: [[-1.12907137 -0.01741918]], reward: -1.0, 100score avg: -196.40322580645162\n",
      "episode: 62/8000, score: 199, e 4.2e-05, help: [[-0.82837594  0.0110193 ]], reward: -1.0, 100score avg: -196.44444444444446\n",
      "episode: 63/8000, score: 199, e 3.6e-05, help: [[-0.04635832  0.04271308]], reward: -1.0, 100score avg: -196.484375\n",
      "episode: 64/8000, score: 199, e 3e-05, help: [[-0.27093903 -0.00339219]], reward: -1.0, 100score avg: -196.52307692307693\n",
      "episode: 65/8000, score: 199, e 2.6e-05, help: [[-0.25205473  0.02023   ]], reward: -1.0, 100score avg: -196.56060606060606\n",
      "episode: 66/8000, score: 171, e 2.2e-05, help: [[0.51201971 0.04244284]], reward: -1.0, 100score avg: -196.17910447761193\n",
      "episode: 67/8000, score: 199, e 1.9e-05, help: [[-0.2661026   0.00632634]], reward: -1.0, 100score avg: -196.22058823529412\n",
      "episode: 68/8000, score: 199, e 1.6e-05, help: [[0.00423577 0.02770633]], reward: -1.0, 100score avg: -196.2608695652174\n",
      "episode: 69/8000, score: 199, e 1.3e-05, help: [[-0.2121263  -0.00254084]], reward: -1.0, 100score avg: -196.3\n",
      "episode: 70/8000, score: 199, e 1.1e-05, help: [[-0.79329438  0.02378909]], reward: -1.0, 100score avg: -196.33802816901408\n",
      "episode: 71/8000, score: 199, e 9.7e-06, help: [[-0.0973763   0.01235354]], reward: -1.0, 100score avg: -196.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 72/8000, score: 190, e 9.7e-06, help: [[0.5367891  0.04001239]], reward: -1.0, 100score avg: -196.2876712328767\n",
      "episode: 73/8000, score: 199, e 9.7e-06, help: [[-0.57902815 -0.00251246]], reward: -1.0, 100score avg: -196.32432432432432\n",
      "episode: 74/8000, score: 199, e 9.7e-06, help: [[-0.29781558  0.00279252]], reward: -1.0, 100score avg: -196.36\n",
      "episode: 75/8000, score: 199, e 9.7e-06, help: [[0.27736617 0.04108927]], reward: -1.0, 100score avg: -196.39473684210526\n",
      "episode: 76/8000, score: 199, e 9.7e-06, help: [[-0.0883658   0.03803891]], reward: -1.0, 100score avg: -196.42857142857142\n",
      "episode: 77/8000, score: 199, e 9.7e-06, help: [[-1.19326692  0.00449118]], reward: -1.0, 100score avg: -196.46153846153845\n",
      "episode: 78/8000, score: 199, e 9.7e-06, help: [[-0.72128972  0.0383005 ]], reward: -1.0, 100score avg: -196.49367088607596\n",
      "episode: 79/8000, score: 199, e 9.7e-06, help: [[-0.56437877  0.01953763]], reward: -1.0, 100score avg: -196.525\n",
      "episode: 80/8000, score: 189, e 9.7e-06, help: [[0.54105686 0.04816836]], reward: -1.0, 100score avg: -196.4320987654321\n",
      "episode: 81/8000, score: 199, e 9.7e-06, help: [[-0.14968465 -0.01296246]], reward: -1.0, 100score avg: -196.46341463414635\n",
      "episode: 82/8000, score: 199, e 9.7e-06, help: [[-0.82025537  0.01328507]], reward: -1.0, 100score avg: -196.49397590361446\n",
      "episode: 83/8000, score: 199, e 9.7e-06, help: [[-0.59299833  0.00939018]], reward: -1.0, 100score avg: -196.52380952380952\n",
      "episode: 84/8000, score: 199, e 9.7e-06, help: [[-0.36214245 -0.01729023]], reward: -1.0, 100score avg: -196.5529411764706\n",
      "episode: 85/8000, score: 199, e 9.7e-06, help: [[-0.95450433  0.00525792]], reward: -1.0, 100score avg: -196.58139534883722\n",
      "episode: 86/8000, score: 199, e 9.7e-06, help: [[-0.39471462  0.03232464]], reward: -1.0, 100score avg: -196.60919540229884\n",
      "episode: 87/8000, score: 199, e 9.7e-06, help: [[-0.69549561  0.00344289]], reward: -1.0, 100score avg: -196.63636363636363\n",
      "episode: 88/8000, score: 190, e 9.7e-06, help: [[0.50847327 0.02160502]], reward: -1.0, 100score avg: -196.56179775280899\n",
      "episode: 89/8000, score: 199, e 9.7e-06, help: [[-0.46015832 -0.04453621]], reward: -1.0, 100score avg: -196.5888888888889\n",
      "episode: 90/8000, score: 199, e 9.7e-06, help: [[-1.04664071  0.01373441]], reward: -1.0, 100score avg: -196.6153846153846\n",
      "episode: 91/8000, score: 199, e 9.7e-06, help: [[-1.01327162  0.03139032]], reward: -1.0, 100score avg: -196.6413043478261\n",
      "episode: 92/8000, score: 117, e 9.7e-06, help: [[0.50731217 0.01194769]], reward: -1.0, 100score avg: -195.78494623655914\n",
      "episode: 93/8000, score: 193, e 9.7e-06, help: [[0.51564991 0.03099604]], reward: -1.0, 100score avg: -195.75531914893617\n",
      "episode: 94/8000, score: 177, e 9.7e-06, help: [[0.50377368 0.02053001]], reward: -1.0, 100score avg: -195.55789473684212\n",
      "episode: 95/8000, score: 199, e 9.7e-06, help: [[0.26575654 0.04960964]], reward: -1.0, 100score avg: -195.59375\n",
      "episode: 96/8000, score: 199, e 9.7e-06, help: [[-0.9330638  -0.01147297]], reward: -1.0, 100score avg: -195.62886597938143\n",
      "episode: 97/8000, score: 199, e 9.7e-06, help: [[-0.58836816  0.02364988]], reward: -1.0, 100score avg: -195.66326530612244\n",
      "episode: 98/8000, score: 199, e 9.7e-06, help: [[ 0.20633655 -0.00258608]], reward: -1.0, 100score avg: -195.6969696969697\n",
      "episode: 99/8000, score: 160, e 9.7e-06, help: [[0.53712315 0.04785519]], reward: -1.0, 100score avg: -195.34\n",
      "episode: 100/8000, score: 199, e 9.7e-06, help: [[-0.5605644  -0.00583576]], reward: -1.0, 100score avg: -195.34\n",
      "episode: 101/8000, score: 193, e 9.7e-06, help: [[0.50062996 0.01189378]], reward: -1.0, 100score avg: -195.28\n",
      "episode: 102/8000, score: 199, e 9.7e-06, help: [[-0.44350146 -0.0098979 ]], reward: -1.0, 100score avg: -195.28\n",
      "episode: 103/8000, score: 199, e 9.7e-06, help: [[-0.98839168  0.03088104]], reward: -1.0, 100score avg: -195.28\n",
      "episode: 104/8000, score: 181, e 9.7e-06, help: [[0.51292055 0.02326108]], reward: -1.0, 100score avg: -195.1\n",
      "episode: 105/8000, score: 199, e 9.7e-06, help: [[ 0.11674738 -0.01508655]], reward: -1.0, 100score avg: -195.1\n",
      "episode: 106/8000, score: 199, e 9.7e-06, help: [[-0.44212313  0.04991527]], reward: -1.0, 100score avg: -195.1\n",
      "episode: 107/8000, score: 199, e 9.7e-06, help: [[-1.05588873 -0.04512976]], reward: -1.0, 100score avg: -195.1\n",
      "episode: 108/8000, score: 199, e 9.7e-06, help: [[0.14852395 0.0105614 ]], reward: -1.0, 100score avg: -195.1\n",
      "episode: 109/8000, score: 196, e 9.7e-06, help: [[0.52262503 0.02879987]], reward: -1.0, 100score avg: -195.07\n",
      "episode: 110/8000, score: 199, e 9.7e-06, help: [[-0.48414083 -0.0052431 ]], reward: -1.0, 100score avg: -195.07\n",
      "episode: 111/8000, score: 199, e 9.7e-06, help: [[-0.32948812 -0.02294141]], reward: -1.0, 100score avg: -195.07\n",
      "episode: 112/8000, score: 176, e 9.7e-06, help: [[0.512135   0.02578618]], reward: -1.0, 100score avg: -194.84\n",
      "episode: 113/8000, score: 105, e 9.7e-06, help: [[0.51342472 0.0168633 ]], reward: -1.0, 100score avg: -193.9\n",
      "episode: 114/8000, score: 178, e 9.7e-06, help: [[0.50362166 0.02039107]], reward: -1.0, 100score avg: -193.69\n",
      "episode: 115/8000, score: 199, e 9.7e-06, help: [[-0.83633028  0.03860865]], reward: -1.0, 100score avg: -193.69\n",
      "episode: 116/8000, score: 180, e 9.7e-06, help: [[0.52278736 0.03310618]], reward: -1.0, 100score avg: -193.5\n",
      "episode: 117/8000, score: 199, e 9.7e-06, help: [[-0.39293586  0.00986802]], reward: -1.0, 100score avg: -193.5\n",
      "episode: 118/8000, score: 199, e 9.7e-06, help: [[-0.3503513   0.01864826]], reward: -1.0, 100score avg: -193.5\n",
      "episode: 119/8000, score: 199, e 9.7e-06, help: [[-1.16110514 -0.00683628]], reward: -1.0, 100score avg: -193.5\n",
      "episode: 120/8000, score: 132, e 9.7e-06, help: [[0.5324036  0.03636887]], reward: -1.0, 100score avg: -192.83\n",
      "episode: 121/8000, score: 199, e 9.7e-06, help: [[-0.77650826 -0.0403359 ]], reward: -1.0, 100score avg: -192.83\n",
      "episode: 122/8000, score: 199, e 9.7e-06, help: [[-0.72519064  0.03703474]], reward: -1.0, 100score avg: -192.83\n",
      "episode: 123/8000, score: 199, e 9.7e-06, help: [[-0.65972641 -0.04314616]], reward: -1.0, 100score avg: -192.83\n",
      "episode: 124/8000, score: 154, e 9.7e-06, help: [[0.50096254 0.0117749 ]], reward: -1.0, 100score avg: -192.38\n",
      "episode: 125/8000, score: 199, e 9.7e-06, help: [[-0.11072542 -0.01270263]], reward: -1.0, 100score avg: -192.38\n",
      "episode: 126/8000, score: 199, e 9.7e-06, help: [[-0.53592566  0.02975366]], reward: -1.0, 100score avg: -192.38\n",
      "episode: 127/8000, score: 199, e 9.7e-06, help: [[-0.17499495  0.02390831]], reward: -1.0, 100score avg: -192.38\n",
      "episode: 128/8000, score: 199, e 9.7e-06, help: [[-0.37330151  0.02642038]], reward: -1.0, 100score avg: -192.38\n",
      "episode: 129/8000, score: 199, e 9.7e-06, help: [[0.20317562 0.01216876]], reward: -1.0, 100score avg: -192.38\n",
      "episode: 130/8000, score: 199, e 9.7e-06, help: [[-0.05108091 -0.01010706]], reward: -1.0, 100score avg: -192.38\n",
      "episode: 131/8000, score: 199, e 9.7e-06, help: [[-0.36097972 -0.02788423]], reward: -1.0, 100score avg: -192.38\n",
      "episode: 132/8000, score: 199, e 9.7e-06, help: [[-0.4384054   0.01901226]], reward: -1.0, 100score avg: -192.38\n",
      "episode: 133/8000, score: 199, e 9.7e-06, help: [[-1.01144525 -0.01865336]], reward: -1.0, 100score avg: -192.38\n",
      "episode: 134/8000, score: 92, e 9.7e-06, help: [[0.50219838 0.02186902]], reward: -1.0, 100score avg: -191.31\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "\n",
    "scores_memory = deque(maxlen=100)\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    \n",
    "    # start each episode at beginning state\n",
    "    state = env.reset()\n",
    "    \n",
    "    # transpose state to fit nicely with DL network\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    # iterate over time steps of game\n",
    "    for time in range(5000):\n",
    "        \n",
    "        if e % 50 == 0:\n",
    "            env.render()\n",
    "        \n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # returned values from taking a step forward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # run default prediction model\n",
    "        if len(agent.memory)>batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        \n",
    "        # moved into next state\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            scores_memory.append(time)\n",
    "            scores_avg = np.mean(scores_memory) * - 1.0\n",
    "            print('episode: {}/{}, score: {}, e {:.2}, help: {}, reward: {}, 100score avg: {}'.format(e, n_episodes, time, agent.epsilon, state, reward, scores_avg))\n",
    "            break\n",
    "            \n",
    "    agent.update_target_model()\n",
    "\n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "        \n",
    "    if e % 50 == 0:\n",
    "        agent.save(output_dir + 'weights_final' + '{:04d}'.format(e) + \".hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
