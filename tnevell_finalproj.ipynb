{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import gym # openai to render environment for agent to take action in and receive feedback in\n",
    "import gym.wrappers\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque # for memory of agent\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential # to build neural network for aproximating optimal Q\n",
    "from keras.layers import Dense # theta weights - weights and bias of neurons in between layers of dense network\n",
    "from keras.optimizers import Adam # stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters related to size of state and size of actions\n",
    "state_size = env.observation_space.shape[0]\n",
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter for gradient descent (vary by powers of 2)\n",
    "batch_size = 32\n",
    "n_episodes = 8000\n",
    "output_dir = \"model_output/MountainCar\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # our agent's memory\n",
    "        # prevents us from going over every single event that's happened. Too Slow!\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        \n",
    "        '''hyperparameters''' \n",
    "        self.gamma = 0.99 # discount factor of future rewards\n",
    "        \n",
    "        # two modes of action: \n",
    "            # exploitation: take best possible action based on whats been learned\n",
    "            # exploration: to explore the environment more and find new actions\n",
    "        self.epsilon = 1.0 # initial exploration rate of agent \n",
    "        self.epsilon_decay = 0.85 # slowly shifts from exploring to exploitation\n",
    "        self.epsilon_min = 0.00001 # lowest exploration percent can decay to\n",
    "        \n",
    "        self.learning_rate = 0.001251 # stochastic gradient descent step size\n",
    "        \n",
    "        self.model = self._build_model() # ensures private method can only be used by this particular instance of a class\n",
    "        self.target_model = self._build_model()\n",
    "        \n",
    "        self.update_target_model()\n",
    "        \n",
    "    def _build_model(self): # where we define dense neural network for approx Q*\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(24, input_dim = self.state_size, activation=\"relu\")) # hidden layer\n",
    "        model.add(Dense(48, activation=\"relu\")) # hidden layer        \n",
    "        \n",
    "        # output layer: as many neurons as possible actions\n",
    "        model.add(Dense(self.action_size, activation=\"linear\")) # reason for linear: we are directly modeling actions, no abstract probability\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=self.learning_rate)) # mse works for this agent, but cross-entropy might work better for others. mse is not usually first choice\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    # REALLY IMPORTANT: takes in state, action, reward, and next_state at current time step\n",
    "    # to model what will happen in next_state and what reward we can expect to receive \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    # figuring out what action to take based on state\n",
    "    def act(self, state):\n",
    "        '''\n",
    "        explore randomly or exploit information accrued in dense network\n",
    "        as epsilon decays exploit will be more likely\n",
    "        '''\n",
    "        \n",
    "        # explore\n",
    "        if np.random.rand(1) <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        #exploit\n",
    "        act_values = self.model.predict(state) # use theta weights and predict method on our model inside the agent to guess best course of action to maz future reward\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    # bulk of agent defs\n",
    "    def replay(self, batch_size):\n",
    "        \n",
    "        # samples from the deque of memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # if reached max time steps or by dying, then done\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # reward plus estimates of future reward using neural network and next state\n",
    "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)[0]))\n",
    "            \n",
    "            # map maximized future reward to current reward with theta\n",
    "            target_f= self.model.predict(state) # use neural network to estimate target given current state\n",
    "            target_f[0][action]= target # map to future state\n",
    "            \n",
    "            states.append(state[0])\n",
    "            targets.append(target_f[0])\n",
    "            \n",
    "        # train model with X: current state, Y: future reward, \n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "#             # if reached max time steps or by dying, then done\n",
    "#             target = reward\n",
    "#             if not done:\n",
    "#                 # reward plus estimates of future reward using neural network and next state\n",
    "#                 target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            \n",
    "#             # map maximized future reward to current reward with theta\n",
    "#             target_f = self.model.predict(state) # use neural network to estimate target given current state\n",
    "#             target_f[0][action] = target # map to future state\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/8000, score: 199, e 1.0, help: [[-0.57948268  0.01260537]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 1/8000, score: 199, e 0.85, help: [[-0.55856871 -0.00198235]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 2/8000, score: 199, e 0.72, help: [[-0.54974823  0.01699448]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 3/8000, score: 199, e 0.61, help: [[-0.57335211  0.01094781]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 4/8000, score: 199, e 0.52, help: [[-0.24606916  0.02434657]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 5/8000, score: 199, e 0.44, help: [[-0.51362194  0.0121166 ]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 6/8000, score: 199, e 0.38, help: [[-0.53800659  0.01649658]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 7/8000, score: 199, e 0.32, help: [[-0.68426201  0.00870184]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 8/8000, score: 199, e 0.27, help: [[-0.54446483 -0.02584037]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 9/8000, score: 199, e 0.23, help: [[-0.93327898  0.01673974]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 10/8000, score: 199, e 0.2, help: [[-0.42466864 -0.01857437]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 11/8000, score: 199, e 0.17, help: [[-0.20200018  0.00843335]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 12/8000, score: 199, e 0.14, help: [[-0.75488626 -0.00832016]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 13/8000, score: 199, e 0.12, help: [[-0.33587603  0.0079385 ]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 14/8000, score: 199, e 0.1, help: [[-0.18377562  0.05236807]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 15/8000, score: 199, e 0.087, help: [[-0.37567532  0.00170844]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 16/8000, score: 199, e 0.074, help: [[-0.04141315 -0.00550256]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 17/8000, score: 199, e 0.063, help: [[-0.39533572 -0.01293545]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 18/8000, score: 199, e 0.054, help: [[-0.41939449  0.00153263]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 19/8000, score: 199, e 0.046, help: [[-0.80899365 -0.03166184]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 20/8000, score: 199, e 0.039, help: [[-0.76684792 -0.00196148]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 21/8000, score: 199, e 0.033, help: [[-0.4431997  -0.03632293]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 22/8000, score: 199, e 0.028, help: [[-0.38577148  0.03144011]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 23/8000, score: 199, e 0.024, help: [[-0.80404148 -0.02332089]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 24/8000, score: 199, e 0.02, help: [[-0.53683665  0.02060616]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 25/8000, score: 199, e 0.017, help: [[0.19914213 0.01004229]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 26/8000, score: 199, e 0.015, help: [[ 0.09408127 -0.01658129]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 27/8000, score: 199, e 0.012, help: [[-0.55037521 -0.01604301]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 28/8000, score: 199, e 0.011, help: [[-0.93545375 -0.02144173]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 29/8000, score: 199, e 0.009, help: [[-0.30525827 -0.02301325]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 30/8000, score: 199, e 0.0076, help: [[-0.74436899  0.03167263]], reward: -1.0, 100score avg: -199.0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "\n",
    "scores_memory = deque(maxlen=100)\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    \n",
    "    # start each episode at beginning state\n",
    "    state = env.reset()\n",
    "    \n",
    "    # transpose state to fit nicely with DL network\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    # iterate over time steps of game\n",
    "    for time in range(5000):\n",
    "        \n",
    "        if e % 50 == 0:\n",
    "            env.render()\n",
    "        \n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # returned values from taking a step forward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # run default prediction model\n",
    "        if len(agent.memory)>batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        \n",
    "        # moved into next state\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            scores_memory.append(time)\n",
    "            scores_avg = np.mean(scores_memory) * - 1.0\n",
    "            print('episode: {}/{}, score: {}, e {:.2}, help: {}, reward: {}, 100score avg: {}'.format(e, n_episodes, time, agent.epsilon, state, reward, scores_avg))\n",
    "            break\n",
    "            \n",
    "    agent.update_target_model()\n",
    "\n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "        \n",
    "    if e % 50 == 0:\n",
    "        agent.save(output_dir + 'weights_final' + '{:04d}'.format(e) + \".hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
